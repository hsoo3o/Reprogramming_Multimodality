{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed) \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "seed_value = 33\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "\n",
    "from model import *\n",
    "from zero_shot_test import *\n",
    "from data import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "cfg_path = 'config_audio.yaml'\n",
    "cfg = yaml.safe_load(open(cfg_path))\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reprogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label setup\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from preprocessing.audioset import *\n",
    "\n",
    "path = './dataset/audio/audioset/class_labels_indices.csv'\n",
    "\n",
    "gt_classes = get_gt(path)\n",
    "\n",
    "csv_file = './dataset/audio/audioset/balanced_train_segments.csv'\n",
    "\n",
    "root_dir =  \"./dataset/audio/audioset/train\"\n",
    "\n",
    "audio_list, video_list = get_audio_n_video_path(root_dir)\n",
    "train_target = get_target(path,csv_file, audio_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import AudioSetDataset\n",
    "from preprocessing.preprocessing_utils import get_img_preprocess_with_rgb\n",
    "\n",
    "video_process = get_img_preprocess_with_rgb()\n",
    "\n",
    "train_val_dataset = AudioSetDataset(root_dir, device, video_process, clips_per_video=1,audio_mean=-4.2677393, audio_std= 4.56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "dataset_size = len(train_val_dataset)\n",
    "train_size = int(0.9 * dataset_size)  \n",
    "val_size = dataset_size - train_size  \n",
    "\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        pin_memory=False,\n",
    "        sampler=None\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        pin_memory=False,\n",
    "        sampler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source ,target in train_dataloader:\n",
    "    print( source.shape, target.shape)\n",
    "    print(target)\n",
    "    print(source)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('./trained_model/reprogram_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    try:\n",
    "        if name.split('.')[1] == 'audio':\n",
    "            print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "            trainable_params.append(param)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['train_params'] = {}\n",
    "cfg['train_params']['optimizer'] = 'AdamW'\n",
    "cfg['train_params']['init_lr'] = 0.001\n",
    "cfg['train_params']['weight_decay'] = 0.2\n",
    "cfg['train_params']['scheduler'] = 'cosw'\n",
    "cfg['train_params']['temperature'] = 0.05\n",
    "cfg['train_params']['T_max'] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer =  optim.AdamW(trainable_params, lr=cfg['train_params']['init_lr'], weight_decay=cfg['train_params']['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['train_params']['T_max'])\n",
    "# scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_best_path = './trained_model/audio/audioset_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "num_epochs = 1000\n",
    "early_stop = 0\n",
    "min_loss = np.inf\n",
    "print(save_best_path)\n",
    "print('with final logit')\n",
    "temperature = cfg['train_params']['temperature']\n",
    "log = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for i, (source, target) in enumerate(train_dataloader):\n",
    "        \n",
    "        \n",
    "        target_inputs = {\n",
    "            'audio': target.to(device)\n",
    "        }\n",
    "        outputs = model(target_inputs,source.to(device))\n",
    "        \n",
    "        # loss\n",
    "        source_features = outputs['source_' + cfg['source_type']]\n",
    "        target_features = outputs['audio']\n",
    "\n",
    "        # normalized features\n",
    "        source_features = source_features / source_features.norm(dim=1, keepdim=True)\n",
    "        target_features = target_features / target_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature)).exp()\n",
    "        # mid_logit = model.mid_logit\n",
    "        logits =  logit_scale * source_features @ target_features.t()\n",
    "\n",
    "\n",
    "        loss_f = cosine_similarity_loss(logits)\n",
    "\n",
    "\n",
    "        loss = loss_f \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    model.eval()  \n",
    "    total_val_loss = 0.0\n",
    "    for source, target in val_dataloader:\n",
    "\n",
    "        target_inputs = {\n",
    "            'audio': target.to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(target_inputs, source.to(device))\n",
    "            \n",
    "        # loss\n",
    "        source_features = outputs['source_' + cfg['source_type']]\n",
    "        target_features = outputs['audio']\n",
    "\n",
    "        # normalized features\n",
    "        source_features = source_features / source_features.norm(dim=1, keepdim=True)\n",
    "        target_features = target_features / target_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "\n",
    "        logits =logit_scale * source_features @ target_features.t()\n",
    "\n",
    "        loss_f = cosine_similarity_loss(logits)\n",
    "\n",
    "        loss =loss_f \n",
    "        total_val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Curr_LR: {curr_lr}, Train Loss: {total_loss / len(train_dataloader):.4f}, Val Loss: {total_val_loss / len(val_dataloader):.4f}\")\n",
    "    log.append([f\"Epoch {epoch + 1}/{num_epochs}, Curr_LR: {curr_lr}, Train Loss: {total_loss / len(train_dataloader):.4f}, Val Loss: {total_val_loss / len(val_dataloader):.4f}\"])\n",
    "\n",
    "\n",
    "    \n",
    "    if total_val_loss / len(val_dataloader) < min_loss:\n",
    "        min_loss = total_val_loss / len(val_dataloader)\n",
    "        early_stop = 0\n",
    "        torch.save(model, save_best_path)\n",
    "        print('saved best')\n",
    "    else:\n",
    "        early_stop += 1\n",
    "    \n",
    "    if early_stop > 3:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    if cfg['train_params']['scheduler'] == 'cosw':\n",
    "        scheduler.step()\n",
    "    elif cfg['train_params']['scheduler'] =='plateau':\n",
    "        scheduler.step(total_val_loss / len(val_dataloader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 2\n",
    "with open(f'./trained_model/log/{model_num}.txt','w') as f:\n",
    "    for l in log:\n",
    "        f.writelines(str(l) + '\\n')\n",
    "    \n",
    "    f.write(str(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test\n",
    "For zero-shot classification get https://github.com/facebookresearch/ImageBind/blob/main/imagebind/bpe/bpe_simple_vocab_16e6.txt.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load(\"./trained_model/reprogram_trained.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## audioset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.audioset import *\n",
    "\n",
    "\n",
    "audio_dir = './dataset/audio/audioset/test'\n",
    "audio_list = get_audio_pathes(audio_dir)\n",
    "\n",
    "class_label_path = './dataset/audio/audioset/class_labels_indices.csv'\n",
    "csv_file = './dataset/audio/audioset/eval_segments.csv'\n",
    "\n",
    "gt_classes = get_gt(class_label_path)\n",
    "\n",
    "target_list = get_target(class_label_path, csv_file, audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import AudioSetDataset_audio\n",
    "import torch\n",
    "\n",
    "root_dir =  \"./dataset/audio/audioset/test\"\n",
    "test_dataset = AudioSetDataset_audio(root_dir, device, clips_per_video=5,audio_mean=-4.2677393, audio_std= 4.56)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=False,\n",
    "        sampler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_templates = [\n",
    "    'a bad photo of a {}.',\n",
    "    'a photo of many {}.',\n",
    "    'a sculpture of a {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a rendering of a {}.',\n",
    "    'graffiti of a {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a tattoo of a {}.',\n",
    "    'the embroidered {}.',\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a bright photo of a {}.',\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a dark photo of the {}.',\n",
    "    'a drawing of a {}.',\n",
    "    'a photo of my {}.',\n",
    "    'the plastic {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a close-up photo of a {}.',\n",
    "    'a black and white photo of the {}.',\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "    'a sculpture of the {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a cropped photo of a {}.',\n",
    "    'a plastic {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "    'a jpeg corrupted photo of a {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a rendering of the {}.',\n",
    "    'a {} in a video game.',\n",
    "    'a photo of one {}.',\n",
    "    'a doodle of a {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a photo of a {}.',\n",
    "    'the origami {}.',\n",
    "    'the {} in a video game.',\n",
    "    'a sketch of a {}.',\n",
    "    'a doodle of the {}.',\n",
    "    'a origami {}.',\n",
    "    'a low resolution photo of a {}.',\n",
    "    'the toy {}.',\n",
    "    'a rendition of the {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a rendition of a {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a blurry photo of a {}.',\n",
    "    'a cartoon {}.',\n",
    "    'art of a {}.',\n",
    "    'a sketch of the {}.',\n",
    "    'a embroidered {}.',\n",
    "    'a pixelated photo of a {}.',\n",
    "    'itap of the {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a good photo of a {}.',\n",
    "    'a plushie {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of the weird {}.',\n",
    "    'the cartoon {}.',\n",
    "    'art of the {}.',\n",
    "    'a drawing of the {}.',\n",
    "    'a photo of the large {}.',\n",
    "    'a black and white photo of a {}.',\n",
    "    'the plushie {}.',\n",
    "    'a dark photo of a {}.',\n",
    "    'itap of a {}.',\n",
    "    'graffiti of the {}.',\n",
    "    'a toy {}.',\n",
    "    'itap of my {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of a small {}.',\n",
    "    'a tattoo of the {}.',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = './ImageBind/imagebind/bpe/bpe_simple_vocab_16e6.txt.gz'\n",
    "zeroshot_weights = zeroshot_classifier(model.cuda(), gt_classes, text_templates, bpe_path, device)\n",
    "# zeroshot_weights = zeroshot_weights.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_emb = []\n",
    "for target in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        tmp_audio_features = model.forward({'audio': target.to(device)})\n",
    "        all_emb.append(tmp_audio_features['audio'])\n",
    "test_audio_features = torch.concat(all_emb)\n",
    "audio_features_norm = test_audio_features / test_audio_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_audio_features = torch.concat(all_emb)\n",
    "audio_features_norm = test_audio_features / test_audio_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = 100. * audio_features_norm @ zeroshot_weights.T\n",
    "map_multilabel_list(logits.cpu(), target_list)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## esc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label setup\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from preprocessing.esc50 import *\n",
    "\n",
    "path = './dataset/audio/esc50/ESC-50-master/meta/esc50.csv'\n",
    "\n",
    "gt_classes = get_gt(path)\n",
    "\n",
    "root_dir =  \"./dataset/audio/esc50/ESC-50-master/audio/\"\n",
    "\n",
    "audio_list = get_audio_path(root_dir, path)\n",
    "test_target = get_target(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Esc50Dataset\n",
    "\n",
    "\n",
    "test_dataset = Esc50Dataset(root_dir, path, 'cpu', clips_per_video=5,audio_mean=-4.2677393, audio_std= 4.56)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=False,\n",
    "        sampler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = './ImageBind/imagebind/bpe/bpe_simple_vocab_16e6.txt.gz'\n",
    "zeroshot_weights = zeroshot_classifier(model.cuda(), gt_classes, text_templates, bpe_path, device)\n",
    "# zeroshot_weights = zeroshot_weights.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_emb = []\n",
    "for target in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        tmp_audio_features = model.forward({'audio': target.to(device)})\n",
    "        all_emb.append(tmp_audio_features['audio'])\n",
    "test_audio_features = torch.concat(all_emb)\n",
    "audio_features_norm = test_audio_features / test_audio_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = 100. * audio_features_norm @ zeroshot_weights.T\n",
    "top1, top5 = top1_top5_acc(logits,test_target.cuda())\n",
    "print(top1, top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imagebind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pkg_resources import packaging\n",
    "\n",
    "import sys\n",
    "sys.path.append('./ImageBind/')\n",
    "\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# 가비지 컬렉터 호출\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## audioset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.audioset import *\n",
    "\n",
    "\n",
    "audio_dir = './dataset/audio/audioset/test'\n",
    "audio_list = get_audio_pathes(audio_dir)\n",
    "\n",
    "class_label_path = './dataset/audio/audioset/class_labels_indices.csv'\n",
    "csv_file = './dataset/audio/audioset/eval_segments.csv'\n",
    "\n",
    "gt_classes = get_gt(class_label_path)\n",
    "\n",
    "target_list = get_target(class_label_path, csv_file, audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_templates = [\n",
    "    'a bad photo of a {}.',\n",
    "    'a photo of many {}.',\n",
    "    'a sculpture of a {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a rendering of a {}.',\n",
    "    'graffiti of a {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a tattoo of a {}.',\n",
    "    'the embroidered {}.',\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a bright photo of a {}.',\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a dark photo of the {}.',\n",
    "    'a drawing of a {}.',\n",
    "    'a photo of my {}.',\n",
    "    'the plastic {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a close-up photo of a {}.',\n",
    "    'a black and white photo of the {}.',\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "    'a sculpture of the {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a cropped photo of a {}.',\n",
    "    'a plastic {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "    'a jpeg corrupted photo of a {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a rendering of the {}.',\n",
    "    'a {} in a video game.',\n",
    "    'a photo of one {}.',\n",
    "    'a doodle of a {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a photo of a {}.',\n",
    "    'the origami {}.',\n",
    "    'the {} in a video game.',\n",
    "    'a sketch of a {}.',\n",
    "    'a doodle of the {}.',\n",
    "    'a origami {}.',\n",
    "    'a low resolution photo of a {}.',\n",
    "    'the toy {}.',\n",
    "    'a rendition of the {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a rendition of a {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a blurry photo of a {}.',\n",
    "    'a cartoon {}.',\n",
    "    'art of a {}.',\n",
    "    'a sketch of the {}.',\n",
    "    'a embroidered {}.',\n",
    "    'a pixelated photo of a {}.',\n",
    "    'itap of the {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a good photo of a {}.',\n",
    "    'a plushie {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of the weird {}.',\n",
    "    'the cartoon {}.',\n",
    "    'art of the {}.',\n",
    "    'a drawing of the {}.',\n",
    "    'a photo of the large {}.',\n",
    "    'a black and white photo of a {}.',\n",
    "    'the plushie {}.',\n",
    "    'a dark photo of a {}.',\n",
    "    'itap of a {}.',\n",
    "    'graffiti of the {}.',\n",
    "    'a toy {}.',\n",
    "    'itap of my {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of a small {}.',\n",
    "    'a tattoo of the {}.',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zeroshot weights\n",
    "\n",
    "zeroshot_weights = []\n",
    "for classname in tqdm(list(gt_classes)):\n",
    "    text_list =  [template.format(classname) for template in text_templates]\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "\n",
    "        embeddings[ModalityType.TEXT] /= embeddings[ModalityType.TEXT].norm(dim=-1, keepdim=True)\n",
    "        embeddings[ModalityType.TEXT] = embeddings[ModalityType.TEXT].mean(dim=0)\n",
    "        embeddings[ModalityType.TEXT] /= embeddings[ModalityType.TEXT].norm()\n",
    "        zeroshot_weights.append(embeddings[ModalityType.TEXT])\n",
    "    # break\n",
    "\n",
    "zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_emb = []\n",
    "for i in range(0,len(audio_list),10):\n",
    "    \n",
    "\n",
    "      tmp_list = audio_list[i:i+10]\n",
    "\n",
    "\n",
    "      inputs = {\n",
    "          ModalityType.AUDIO: data.load_and_transform_audio_data(tmp_list, device, mean = -4.2677393, std = 4.56),\n",
    "      }\n",
    "\n",
    "      with torch.no_grad():\n",
    "          imembeddings = model(inputs)\n",
    "\n",
    "          all_emb.append(imembeddings[ModalityType.AUDIO] )\n",
    "\n",
    "audio_emb = torch.concat(all_emb)\n",
    "audio_emb /= audio_emb.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = 100. * audio_emb @ zeroshot_weights\n",
    "map_multilabel_list(logits.cpu(), target_list)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## esc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label setup\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from preprocessing.esc50 import *\n",
    "\n",
    "path = './dataset/audio/esc50/ESC-50-master/meta/esc50.csv'\n",
    "\n",
    "gt_classes = get_gt(path)\n",
    "\n",
    "root_dir =  \"./dataset/audio/esc50/ESC-50-master/audio/\"\n",
    "\n",
    "audio_list = get_audio_path(root_dir, path)\n",
    "test_target = get_target(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zeroshot weights\n",
    "\n",
    "zeroshot_weights = []\n",
    "for classname in tqdm(list(gt_classes)):\n",
    "    text_list =  [template.format(classname) for template in text_templates]\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "\n",
    "        embeddings[ModalityType.TEXT] /= embeddings[ModalityType.TEXT].norm(dim=-1, keepdim=True)\n",
    "        embeddings[ModalityType.TEXT] = embeddings[ModalityType.TEXT].mean(dim=0)\n",
    "        embeddings[ModalityType.TEXT] /= embeddings[ModalityType.TEXT].norm()\n",
    "        zeroshot_weights.append(embeddings[ModalityType.TEXT])\n",
    "    # break\n",
    "\n",
    "zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_emb = []\n",
    "for i in range(0,len(audio_list),10):\n",
    "    \n",
    "\n",
    "      tmp_list = audio_list[i:i+10]\n",
    "\n",
    "\n",
    "      inputs = {\n",
    "          ModalityType.AUDIO: data.load_and_transform_audio_data(tmp_list, device, mean = -4.2677393, std = 4.56),\n",
    "      }\n",
    "\n",
    "      with torch.no_grad():\n",
    "          imembeddings = model(inputs)\n",
    "\n",
    "          all_emb.append(imembeddings[ModalityType.AUDIO] )\n",
    "\n",
    "audio_emb = torch.concat(all_emb)\n",
    "audio_emb /= audio_emb.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prediction\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu()) for k in topk]\n",
    "\n",
    "logits = 100. * audio_emb @ zeroshot_weights\n",
    "\n",
    "top1, top5, n = 0., 0., 0.\n",
    "\n",
    "# measure accuracy\n",
    "acc1, acc5 = accuracy(logits, test_target[:9210].to(device), topk=(1, 5))\n",
    "n += test_target.size(0)\n",
    "\n",
    "top1 = (acc1 / n) * 100\n",
    "top5 = (acc5 / n) * 100\n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reprogram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
